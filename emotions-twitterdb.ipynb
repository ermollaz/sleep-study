{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Prediction Notebook\n",
    "\n",
    "This notebook guides you through building and evaluating an emotion prediction model from text data. It includes:\n",
    "\n",
    "- **Data Preparation:** Loading a CSV of texts with emotion labels, balancing the dataset, and generating text embeddings.\n",
    "- **Model Development:** Initializing the model, defining its architecture, training it, and evaluating its performance.\n",
    "- **Prediction & Visualization:** Running predictions on sample texts and visualizing the results using polar charts.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Initial Setup\n",
    "This section imports all the necessary libraries and modules, including utilities for data handling, embedding generation, and model evaluation. It also defines several helper functions and data models using Pydantic. Additionally, paths, filenames, and parameters for the dataset and model configuration are set up. Key tasks include:\n",
    "- Reading a CSV file with emotion labels and texts.\n",
    "- Preparing a balanced dataset by sampling equal numbers of examples per label.\n",
    "- Defining data models (`Run`, `Report`, `Task`) to structure the data.\n",
    "- Setting file paths for intermediate outputs (embeddings, labels) and mapping conditions and colors.\n",
    "- Loading previously saved tasks and reports from pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Imports\n",
    "#####\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import plotly.graph_objects as go\n",
    "import utils\n",
    "from importlib import reload\n",
    "from utils import generate_embedding\n",
    "import filter\n",
    "# Load your CSV file containing emotion labels and texts.\n",
    "def prep_data(training_data, samples):\n",
    "    df = pd.read_csv(training_data)\n",
    "\n",
    "    # Determine the number of unique labels.\n",
    "    unique_labels = df['label'].unique()\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    # Compute the number of samples per label.\n",
    "    samples_per_label = samples // num_labels\n",
    "\n",
    "    # Use groupby and sample to get a balanced dataset.\n",
    "    balanced_df = df.groupby('label', group_keys=False).apply(\n",
    "    lambda group: group.sample(n=samples_per_label, random_state=42)\n",
    "    )\n",
    "\n",
    "# Extract texts and labels.\n",
    "    balanced_texts = balanced_df['text'].tolist()\n",
    "    balanced_labels = balanced_df['label'].tolist()\n",
    "    return balanced_texts,balanced_labels\n",
    "#####\n",
    "# Datamodels\n",
    "#####\n",
    "class Run(BaseModel):\n",
    "    run_number: int\n",
    "    report: str\n",
    "    embeddings: Optional[List[float]] = None\n",
    "\n",
    "class Report(BaseModel):\n",
    "    name: str\n",
    "    run: List[Run]\n",
    "    condition: List[str]\n",
    "\n",
    "class Task(BaseModel):\n",
    "    task_name: str\n",
    "    description: str\n",
    "    embeddings: Optional[List[float]] = None\n",
    "\n",
    "#####\n",
    "# Paths & Naming\n",
    "#####\n",
    "total_samples = 500\n",
    "embedded_reports = \"data/interim/reports.pkl\"\n",
    "embedded_tasks = \"data/interim/tasks.pkl\"\n",
    "training_data = \"data/raw/emotions.csv\"\n",
    "task_names = [\"Gehen\",\"Schreibtisch\",\"Tisch\"]\n",
    "embeddings_path = f\"data/model/input/emotions_embeddings_{total_samples}.pkl\"\n",
    "labels_path = f\"data/model/labels/emotions_labels_{total_samples}.pkl\"\n",
    "conditions_map = {\n",
    "    1: \"complete\",\n",
    "    2: \"incomplete\",\n",
    "    3: \"interrupted\"\n",
    "}\n",
    "colors = [\n",
    "    \"rgba(255, 0, 0, 0.6)\", # red\n",
    "    \"rgba(0, 255, 0, 0.6)\", # green\n",
    "    \"rgba(0, 0, 255, 0.6)\" # blue\n",
    "    ]\n",
    "# Define emotion names (order must match the predictor's output).\n",
    "emotion_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "with open(embedded_tasks, \"rb\") as f:\n",
    "    tasks_data = pickle.load(f)\n",
    "tasks = [Task(**data) for data in tasks_data]\n",
    "\n",
    "with open(embedded_reports, \"rb\") as f:\n",
    "    report_data = pickle.load(f)\n",
    "reports = [Report(**data) for data in report_data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Embedding Generation\n",
    "This part loads the balanced texts and labels using the earlier defined helper function. It checks if the embeddings and labels already exist in specified files; if not, it generates embeddings for the texts using a batch processing function from the utilities module and saves them. The same is done for the labels:\n",
    "- Reading and balancing the dataset from the CSV file.\n",
    "- Generating text embeddings if they are not already computed.\n",
    "- Saving the generated embeddings and labels to disk for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_texts, balanced_labels = prep_data(training_data=training_data,samples=total_samples)\n",
    "if os.path.exists(embeddings_path):\n",
    "    with open(embeddings_path, \"rb\") as f:\n",
    "        emotions_embeddings = pickle.load(f)\n",
    "else:\n",
    "    # Generate embeddings for the balanced texts.\n",
    "    emotions_embeddings = await utils.batch_generate_embeddings(balanced_texts)\n",
    "    with open(embeddings_path, \"wb\") as f:\n",
    "        pickle.dump(emotions_embeddings, f)\n",
    "\n",
    "if os.path.exists(labels_path):\n",
    "    with open(labels_path, \"rb\") as f:\n",
    "        emotions_labels = pickle.load(f)\n",
    "else:    \n",
    "    # Save the labels to a file.\n",
    "    with open(labels_path, \"wb\") as f:\n",
    "        pickle.dump(balanced_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model Initialization, Architecture Definition, Training, and Evaluation\n",
    "Here, the emotion prediction model is set up. The workflow includes:\n",
    "- Instantiating the `EmotionPredictor` with the computed embeddings and corresponding labels.\n",
    "- Defining the model architecture with specified hidden layer sizes.\n",
    "- Training the model using a predefined number of epochs and learning rate.\n",
    "- Evaluating the model's performance on a test set after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize the EmotionPredictor with the embeddings and labels.\n",
    "predictor = filter.EmotionPredictor(emotions_embeddings, balanced_labels)\n",
    "\n",
    "# 4. Define the model architecture.\n",
    "predictor.define_model(hidden_size1=500, hidden_size2=150, hidden_size3=25, model_path=\"./emotions_model.pth\")\n",
    "\n",
    "# 5. Train the model.\n",
    "predictor.train_model(epochs=200, lr=0.01)\n",
    "\n",
    "# 6. Evaluate the model on the test set.\n",
    "predictor.evaluate_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Prediction Example\n",
    "This segment demonstrates how to use the trained model to predict the emotion probabilities for a specific text report:\n",
    "- An embedding from a specific run in the reports data is extracted.\n",
    "- The predictor uses this embedding to generate probabilities for each emotion.\n",
    "- The predicted probabilities are printed alongside the corresponding text report, mapping each value to a particular emotion (e.g., sadness, joy, love, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding = reports[2].run[1].embeddings\n",
    "probabilities = predictor.predict(embedding)\n",
    "print(f\"\"\"\n",
    "Predicted probabilities for each emotion for the following text:\n",
    "{reports[2].run[1].report}\n",
    "\n",
    "sadness: {probabilities[0][0]}, \n",
    "joy: {probabilities[0][1]},\n",
    "love: {probabilities[0][2]},\n",
    "anger: {probabilities[0][3]},\n",
    "fear: {probabilities[0][4]},\n",
    "surprise: {probabilities[0][5]}  \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Aggregating Predictions by Condition\n",
    "In this cell, the code iterates over the report data to collect and aggregate the emotion prediction probabilities based on different conditions (e.g., \"complete\", \"incomplete\", \"interrupted\"):\n",
    "- For each report, the corresponding condition is identified.\n",
    "- For each run within a report, if valid, the prediction probabilities are computed.\n",
    "- These probabilities are grouped in a dictionary keyed by the condition, setting the stage for later analysis and visualization.\n",
    "- An array of emotion names is prepared and the conditions are sorted for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to collect probability distributions per condition.\n",
    "# Key: condition (e.g. \"Condition complete\"), Value: list of probability arrays for each run.\n",
    "condition_probabilities = {}\n",
    "\n",
    "for report in reports:\n",
    "    # If you want to associate a run with only one condition (say, the first)\n",
    "    cond_key = f\"Condition {report.condition[0]}\"\n",
    "    if cond_key not in condition_probabilities:\n",
    "        condition_probabilities[cond_key] = []\n",
    "        \n",
    "    for run in report.run:\n",
    "        if run.embeddings is not None and run.report.strip() != \"\":\n",
    "            emb = np.array(run.embeddings)\n",
    "            probs = predictor.predict(emb)  # This returns a probability distribution\n",
    "            # Append the probabilities to the list for this condition\n",
    "            condition_probabilities[cond_key].append(probs)\n",
    "angles = emotion_names + [emotion_names[0]]\n",
    "sorted_conditions = sorted(condition_probabilities.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizing Mean and Variability of Emotion Predictions\n",
    "The final section creates a polar chart visualization using Plotly to display the average emotion probabilities and their variability (standard deviation) for each condition:\n",
    "- Subplots are generated for each condition.\n",
    "- For each condition, the mean and standard deviation of the probabilities across runs are calculated.\n",
    "- A shaded polygon (±1 standard deviation) is added to the plot for each condition to visually represent the variability.\n",
    "- The mean probabilities are plotted as a line with markers on a polar coordinate system.\n",
    "- The layout of the polar axes is adjusted to have a consistent range, ensuring that the visualization is clear and comparable across conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define emotion names (order must match the predictor's output).\n",
    "emotion_names = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
    "# Create angles; repeat the first emotion to close the polygon.\n",
    "angles = emotion_names + [emotion_names[0]]\n",
    "sorted_conditions = sorted(condition_probabilities.keys())\n",
    "\n",
    "# Create a subplot with one polar chart per condition.\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(sorted_conditions),\n",
    "    specs=[[{'type': 'polar'}] * len(sorted_conditions)],\n",
    "    subplot_titles=[f\"{cond}\" for cond in sorted_conditions]\n",
    ")\n",
    "\n",
    "# Define colors for conditions.\n",
    "colors = {\n",
    "    sorted_conditions[0]: \"red\",\n",
    "    sorted_conditions[1]: \"blue\",\n",
    "    sorted_conditions[2]: \"green\"\n",
    "}\n",
    "\n",
    "for i, cond in enumerate(sorted_conditions):\n",
    "    # Convert the list of probability arrays to a NumPy array.\n",
    "    data = np.array(condition_probabilities[cond])\n",
    "    \n",
    "    # If the data has an extra dimension (e.g., shape (n_runs, 2, n_emotions))\n",
    "    # and the two rows are identical, select the first row.\n",
    "    if data.ndim == 3 and data.shape[1] == 2:\n",
    "        data = data[:, 0, :]  # Now data shape becomes (n_runs, n_emotions)\n",
    "    \n",
    "    # Compute mean and standard deviation for each emotion.\n",
    "    mean_values = data.mean(axis=0)   # shape: (n_emotions,)\n",
    "    std_values = data.std(axis=0)     # shape: (n_emotions,)\n",
    "    \n",
    "    # Close the polygons by appending the first value at the end.\n",
    "    mean_closed = np.concatenate([mean_values[0], [mean_values[0][0]]])\n",
    "    # Compute the upper and lower bounds for the fill.\n",
    "    upper_bound = mean_values + std_values\n",
    "    lower_bound = mean_values - std_values\n",
    "    # Close the bounds.\n",
    "    upper_closed = np.concatenate([upper_bound[0], [upper_bound[0][0]]])\n",
    "    lower_closed = np.concatenate([lower_bound[0], [lower_bound[0][0]]])\n",
    "    \n",
    "    # Build a polygon for the shaded area (upper bound then reversed lower bound).\n",
    "    fill_r = np.concatenate([upper_closed, lower_closed[::-1]])\n",
    "    fill_theta = np.concatenate([angles, angles[::-1]])\n",
    "    \n",
    "    # Add the shaded area for ±1 standard deviation.\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=fill_r,\n",
    "        theta=fill_theta,\n",
    "        fill='toself',\n",
    "        fillcolor=colors.get(cond, \"black\"),\n",
    "        opacity=0.2,\n",
    "        line=dict(color='rgba(0,0,0,0)'),\n",
    "        showlegend=False,\n",
    "        name=f'{cond} Std'\n",
    "    ), row=1, col=i+1)\n",
    "    \n",
    "    # Add the mean line.\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=mean_closed,\n",
    "        theta=angles,\n",
    "        mode='lines+markers',\n",
    "        name=f'{cond} Mean',\n",
    "        line=dict(color=colors.get(cond, \"black\"))\n",
    "    ), row=1, col=i+1)\n",
    "\n",
    "# Let Plotly auto-scale the radial axis or set an appropriate range.\n",
    "for i in range(1, len(sorted_conditions) + 1):\n",
    "    polar_id = f\"polar{i}\" if i > 1 else \"polar\"\n",
    "    fig.update_layout({\n",
    "        polar_id: dict(\n",
    "            radialaxis=dict(\n",
    "                range=[0, 1],  # Adjust this range if needed.\n",
    "                autorange=False\n",
    "            )\n",
    "        )\n",
    "    })\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
