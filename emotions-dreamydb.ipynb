{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dreamy\n",
    "**Pretty much like the twitter db notebook but this uses the databank and the dreamy [repository](https://github.com/lorenzoscottb/DReAMy) to make predictions instead of training our own neural network. It relies on a pretrained BERT model using [labeled data](https://dreambank.net/) (available on request)**\n",
    "\n",
    "cite as:\n",
    "```\n",
    "@article{BERTOLINI2024406,\n",
    "title = {DReAMy: a library for the automatic analysis and annotation of dream reports with multilingual large language models},\n",
    "journal = {Sleep Medicine},\n",
    "volume = {115},\n",
    "pages = {406-407},\n",
    "year = {2024},\n",
    "note = {Abstracts from the 17th World Sleep Congress},\n",
    "issn = {1389-9457},\n",
    "doi = {https://doi.org/10.1016/j.sleep.2023.11.1092},\n",
    "url = {https://www.sciencedirect.com/science/article/pii/S1389945723015186},\n",
    "author = {L. Bertolini and A. Michalak and J. Weeds}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# Imports\n",
    "#####\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.graph_objects as go\n",
    "import dreamy\n",
    "label_dict = {\n",
    "    \"AP\": \"apprehension\",\n",
    "    \"CO\": \"confusion\",\n",
    "    \"HA\": \"happiness\",\n",
    "    \"AN\": \"anger\",\n",
    "    \"SD\": \"sadness\"\n",
    "}\n",
    "def dreamy_pred(report):\n",
    "    task        = \"SA\"\n",
    "    batch_size = 16\n",
    "    device     = \"cpu\"  # or \"cuda\" / device number (e.g., 0) for GPU\n",
    "    SA_predictions = dreamy.annotate_reports(\n",
    "        [report], \n",
    "        task=task, \n",
    "        device=device,\n",
    "        batch_size=batch_size, \n",
    "    )\n",
    "    SA_predictions[0][0]\n",
    "    return [entry['score'] for entry in SA_predictions[0]]\n",
    "# Load your CSV file containing emotion labels and texts.\n",
    "def prep_data(training_data, samples):\n",
    "    df = pd.read_csv(training_data)\n",
    "\n",
    "    # Determine the number of unique labels.\n",
    "    unique_labels = df['label'].unique()\n",
    "    num_labels = len(unique_labels)\n",
    "\n",
    "    # Compute the number of samples per label.\n",
    "    samples_per_label = samples // num_labels\n",
    "\n",
    "    # Use groupby and sample to get a balanced dataset.\n",
    "    balanced_df = df.groupby('label', group_keys=False).apply(\n",
    "    lambda group: group.sample(n=samples_per_label, random_state=42)\n",
    "    )\n",
    "\n",
    "# Extract texts and labels.\n",
    "    balanced_texts = balanced_df['text'].tolist()\n",
    "    balanced_labels = balanced_df['label'].tolist()\n",
    "    return balanced_texts,balanced_labels\n",
    "#####\n",
    "# Datamodels\n",
    "#####\n",
    "class Run(BaseModel):\n",
    "    run_number: int\n",
    "    report: str\n",
    "    embeddings: Optional[List[float]] = None\n",
    "\n",
    "class Report(BaseModel):\n",
    "    name: str\n",
    "    run: List[Run]\n",
    "    condition: List[str]\n",
    "\n",
    "class Task(BaseModel):\n",
    "    task_name: str\n",
    "    description: str\n",
    "    embeddings: Optional[List[float]] = None\n",
    "\n",
    "#####\n",
    "# Paths & Naming\n",
    "#####\n",
    "total_samples = 500\n",
    "embedded_reports = \"data/interim/reports.pkl\"\n",
    "embedded_tasks = \"data/interim/tasks.pkl\"\n",
    "training_data = \"data/raw/emotions.csv\"\n",
    "task_names = [\"Gehen\",\"Schreibtisch\",\"Tisch\"]\n",
    "embeddings_path = f\"data/model/input/emotions_embeddings_{total_samples}.pkl\"\n",
    "labels_path = f\"data/model/labels/emotions_labels_{total_samples}.pkl\"\n",
    "conditions_map = {\n",
    "    1: \"complete\",\n",
    "    2: \"incomplete\",\n",
    "    3: \"interrupted\"\n",
    "}\n",
    "colors = [\n",
    "    \"rgba(255, 0, 0, 0.6)\", # red\n",
    "    \"rgba(0, 255, 0, 0.6)\", # green\n",
    "    \"rgba(0, 0, 255, 0.6)\" # blue\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(embedded_tasks, \"rb\") as f:\n",
    "    tasks_data = pickle.load(f)\n",
    "tasks = [Task(**data) for data in tasks_data]\n",
    "\n",
    "with open(embedded_reports, \"rb\") as f:\n",
    "    report_data = pickle.load(f)\n",
    "reports = [Report(**data) for data in report_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to collect probability distributions per condition.\n",
    "# Key: condition (e.g. \"Condition complete\"), Value: list of probability arrays for each run.\n",
    "condition_probabilities = {}\n",
    "\n",
    "for report in reports:\n",
    "    # If you want to associate a run with only one condition (say, the first)\n",
    "    cond_key = f\"Condition {report.condition[0]}\"\n",
    "    if cond_key not in condition_probabilities:\n",
    "        condition_probabilities[cond_key] = []\n",
    "        \n",
    "    for run in report.run:\n",
    "        if run.embeddings is not None and run.report.strip() != \"\":\n",
    "            probs = dreamy_pred(run.report)\n",
    "            # Append the probabilities to the list for this condition\n",
    "            condition_probabilities[cond_key].append(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Define emotion names (order must match the predictor's output).\n",
    "emotion_names = list(label_dict.values())\n",
    "# Create angles; we repeat the first emotion to close the polygon.\n",
    "angles = emotion_names + [emotion_names[0]]\n",
    "sorted_conditions = sorted(condition_probabilities.keys())\n",
    "\n",
    "# Create a subplot with one polar chart per condition.\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=len(sorted_conditions),\n",
    "    specs=[[{'type': 'polar'}] * len(sorted_conditions)],\n",
    "    subplot_titles=[f\"{cond}\" for cond in sorted_conditions]\n",
    ")\n",
    "\n",
    "# Define colors for conditions.\n",
    "colors = {\n",
    "    sorted_conditions[0]: \"red\",\n",
    "    sorted_conditions[1]: \"blue\",\n",
    "    sorted_conditions[2]: \"green\"\n",
    "}\n",
    "\n",
    "for i, cond in enumerate(sorted_conditions):\n",
    "    # Convert the list of probability arrays to a NumPy array.\n",
    "    data = np.array(condition_probabilities[cond])\n",
    "    \n",
    "    # If the data has an extra dimension (e.g., shape (n_runs, 2, n_emotions))\n",
    "    # and the two rows are identical, select the first row.\n",
    "    if data.ndim == 3 and data.shape[1] == 2:\n",
    "        data = data[:, 0, :]  # Now data shape becomes (n_runs, n_emotions)\n",
    "    \n",
    "    # Compute mean and standard deviation for each emotion.\n",
    "    mean_values = data.mean(axis=0)   # shape: (n_emotions,)\n",
    "    std_values = data.std(axis=0)     # shape: (n_emotions,)\n",
    "    \n",
    "    # Close the polygons by appending the first value at the end.\n",
    "    mean_closed = np.concatenate([mean_values, [mean_values[0]]])\n",
    "    # Compute the upper and lower bounds for the fill.\n",
    "    upper_bound = mean_values + std_values\n",
    "    lower_bound = mean_values - std_values\n",
    "    # Close the bounds.\n",
    "    upper_closed = np.concatenate([upper_bound, [upper_bound[0]]])\n",
    "    lower_closed = np.concatenate([lower_bound, [lower_bound[0]]])\n",
    "    \n",
    "    # Build a polygon for the shaded area (upper bound then reversed lower bound).\n",
    "    fill_r = np.concatenate([upper_closed, lower_closed[::-1]])\n",
    "    fill_theta = np.concatenate([angles, angles[::-1]])\n",
    "    \n",
    "    # Add the shaded area for Â±1 standard deviation.\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=fill_r,\n",
    "        theta=fill_theta,\n",
    "        fill='toself',\n",
    "        fillcolor=colors.get(cond, \"black\"),\n",
    "        opacity=0.2,\n",
    "        line=dict(color='rgba(0,0,0,0)'),\n",
    "        showlegend=False,\n",
    "        name=f'{cond} Std'\n",
    "    ), row=1, col=i+1)\n",
    "    \n",
    "    # Add the mean line.\n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=mean_closed,\n",
    "        theta=angles,\n",
    "        mode='lines+markers',\n",
    "        name=f'{cond} Mean',\n",
    "        line=dict(color=colors.get(cond, \"black\"))\n",
    "    ), row=1, col=i+1)\n",
    "\n",
    "# Let Plotly auto-scale the radial axis or set an appropriate range.\n",
    "for i in range(1, len(sorted_conditions) + 1):\n",
    "    polar_id = f\"polar{i}\" if i > 1 else \"polar\"\n",
    "    fig.update_layout({\n",
    "        polar_id: dict(\n",
    "            radialaxis=dict(\n",
    "                range=[0, 1],  # Adjust this range if needed.\n",
    "                autorange=False\n",
    "            )\n",
    "        )\n",
    "    })\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
